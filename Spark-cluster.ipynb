{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "Dal Magro Matteo\n",
    "\n",
    "Rosset Lorenzo\n",
    "\n",
    "Zanola Andrea\n",
    "\n",
    "## Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, to_json, col, when, sum, count, struct, collect_list\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME=/usr/local/spark/\n",
      "PYSPARK_PYTHON=/usr/bin/python3.6\n",
      "PATH=/usr/bin:/root/anaconda3/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/spark/bin:/root/bin\n"
     ]
    }
   ],
   "source": [
    "findspark.init('/usr/local/spark/')\n",
    "# check some of the env variables\n",
    "!env | grep -i spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark//logs/spark-root-org.apache.spark.deploy.master.Master-1-mapd-b-gr07-1.novalocal.out\n",
      "spark-master: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr07-1.novalocal.out\n",
      "spark-slave: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr07-2.novalocal.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr07-3.novalocal.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr07-4.novalocal.out\n"
     ]
    }
   ],
   "source": [
    "# Start Spark cluster\n",
    "!$SPARK_HOME/sbin/start-all.sh --host localhost --port 7077 --webui-port 8080\n",
    "!ssh kafka-broker1 /usr/local/spark/sbin/start-worker.sh spark://10.67.22.193:7077 --cores 2 --memory 2g\n",
    "!ssh kafka-broker2 /usr/local/spark/sbin/start-worker.sh spark://10.67.22.193:7077 --cores 2 --memory 2g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_cluster</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1ad08d6f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .appName(\"spark_cluster\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Kafka and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = 'kafka_2.13-2.8.0'\n",
    "KAFKA_BOOTSTRAP_SERVERS = '10.67.22.127:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'topic_stream')\\\n",
    "    .option(\"startingOffsets\", \"latest\")\\\n",
    "    .load()\n",
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\", StringType()),\n",
    "                StructField(\"FPGA\", StringType()),\n",
    "                StructField(\"TDC_CHANNEL\", StringType()),\n",
    "                StructField(\"ORBIT_CNT\", StringType()),\n",
    "                StructField(\"BX_COUNTER\", StringType()),\n",
    "                StructField(\"TDC_MEAS\", StringType())\n",
    "        ]\n",
    ")\n",
    "\n",
    "df = inputDF\\\n",
    "    .select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))\\\n",
    "    .select('value.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: integer (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"HEAD\", col(\"HEAD\").cast('integer')) \\\n",
    "    .withColumn(\"FPGA\", col(\"FPGA\").cast('integer')) \\\n",
    "    .withColumn(\"TDC_CHANNEL\", col(\"TDC_CHANNEL\").cast('integer')) \\\n",
    "    .withColumn(\"ORBIT_CNT\", col(\"ORBIT_CNT\").cast('integer')) \\\n",
    "    .withColumn(\"BX_COUNTER\", col(\"BX_COUNTER\").cast('integer')) \\\n",
    "    .withColumn(\"TDC_MEAS\", col(\"TDC_MEAS\").cast('double'))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the `chamber` and `ABSOLUTETIME` columns. Chamber number 4 refers to the scintillator signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: integer (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      " |-- chamber: integer (nullable = true)\n",
      " |-- ABSOLUTETIME: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    df\n",
    "    .filter(col('HEAD') == 2)\n",
    "    .withColumn('chamber',\n",
    "                when((df.FPGA == 0) & (df.TDC_CHANNEL <= 63), 0)\n",
    "               .when((df.FPGA == 0) & (df.TDC_CHANNEL > 63) & (df.TDC_CHANNEL <= 127), 1)\n",
    "               .when((df.FPGA == 1) & (df.TDC_CHANNEL <= 63), 2)\n",
    "               .when((df.FPGA == 1) & (df.TDC_CHANNEL > 63) & (df.TDC_CHANNEL <= 127), 3)\n",
    "               .when((df.FPGA == 1) & (df.TDC_CHANNEL == 128), 4)\n",
    "               .otherwise(None))\n",
    "    .filter(col('chamber').isNotNull())\n",
    "    .withColumn('ABSOLUTETIME', 25*(col('ORBIT_CNT')*3564 + col('BX_COUNTER') + col('TDC_MEAS')/30))\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    msg = {\n",
    "    'batch_id' : 0,\n",
    "    'total_hits' : 0,\n",
    "    'chamber0' : {\n",
    "        'total_hits' : 0,\n",
    "        'hist_CHANNEL' : {\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'hist_ORBIT' : {\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'hist_SCINT' :{\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'DRIFTIME' : []\n",
    "    },\n",
    "    'chamber1' : {\n",
    "        'total_hits' : 0,\n",
    "        'hist_CHANNEL' : {\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'hist_ORBIT' : {\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'hist_SCINT' :{\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'DRIFTIME' : []\n",
    "    },\n",
    "    'chamber2' : {\n",
    "        'total_hits' : 0,\n",
    "        'hist_CHANNEL' : {\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'hist_ORBIT' : {\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'hist_SCINT' :{\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'DRIFTIME' : []\n",
    "    },\n",
    "    'chamber3' : {\n",
    "        'total_hits' : 0,\n",
    "        'hist_CHANNEL' : {\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'hist_ORBIT' : {\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'hist_SCINT' :{\n",
    "            'bin_edges' : [],\n",
    "            'bin_counts' : []\n",
    "        },\n",
    "        'DRIFTIME' : []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scintillator time offset by Chamber\n",
    "time_offset_by_chamber = {\n",
    "    0: 95.0 - 1.1, # Ch 0\n",
    "    1: 95.0 + 6.4, # Ch 1\n",
    "    2: 95.0 + 0.5, # Ch 2\n",
    "    3: 95.0 - 2.6, # Ch 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "def process_batch(df, batch_id):\n",
    "    \n",
    "    # Start time\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    msg['chamber0']['total_hits'] = 0\n",
    "    msg['chamber1']['total_hits'] = 0\n",
    "    msg['chamber2']['total_hits'] = 0\n",
    "    msg['chamber3']['total_hits'] = 0\n",
    "    msg['batch_id'] = batch_id\n",
    "    \n",
    "    # Hit counts for each chamber\n",
    "    df_counts = (\n",
    "        df\n",
    "        .groupBy('chamber')\n",
    "        .count()\n",
    "        .withColumnRenamed('count', 'chamber_hits')\n",
    "    )\n",
    "    \n",
    "    # Histogram of counts for active channels\n",
    "    df_channel = (\n",
    "        df\n",
    "        .filter(col('chamber') != 4)\n",
    "        .groupBy('TDC_CHANNEL', 'chamber')\n",
    "        .count()\n",
    "        .withColumnRenamed('TDC_CHANNEL', 'bin_edges')\n",
    "        .withColumnRenamed('count', 'counts')\n",
    "        .groupBy('chamber')\n",
    "        .agg(\n",
    "            struct(\n",
    "                collect_list('bin_edges').alias('bin_edges'),\n",
    "                collect_list('counts').alias('counts'),\n",
    "            ).alias('hist_CHANNEL')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Histogram of the number of active channels per orbit\n",
    "    df_orbit = (\n",
    "        df\n",
    "        .filter(col('chamber') != 4)\n",
    "        .groupBy('ORBIT_CNT', 'chamber')\n",
    "        .count()\n",
    "        .withColumnRenamed('ORBIT_CNT', 'bin_edges')\n",
    "        .withColumnRenamed('count', 'counts')\n",
    "        .groupBy('chamber')\n",
    "        .agg(\n",
    "            struct(\n",
    "                collect_list('bin_edges').alias('bin_edges'),\n",
    "                collect_list('counts').alias('counts'),\n",
    "            ).alias('hist_ORBIT')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Contains only those ORBIT_CNT compatible with the passage of a muon\n",
    "    df_scint = (\n",
    "        df\n",
    "        .select('chamber', 'ORBIT_CNT', 'ABSOLUTETIME')\n",
    "        .filter(col('chamber') == 4)\n",
    "        .groupBy('ORBIT_CNT')\n",
    "        .min('ABSOLUTETIME')\n",
    "        .withColumnRenamed('min(ABSOLUTETIME)', 't0')\n",
    "    )\n",
    "    \n",
    "    # Filter df keeping only muon events -> histogram of active cannels\n",
    "    df_muons = (\n",
    "       df\n",
    "       .filter(col('chamber') != 4)\n",
    "       .join(df_scint, on='ORBIT_CNT', how='leftsemi')\n",
    "       .groupBy('TDC_CHANNEL', 'chamber')\n",
    "       .count()\n",
    "       .withColumnRenamed('TDC_CHANNEL', 'bin_edges')\n",
    "       .withColumnRenamed('count', 'counts')\n",
    "       .groupBy('chamber')\n",
    "       .agg(\n",
    "           struct(\n",
    "               collect_list('bin_edges').alias('bin_edges'),\n",
    "               collect_list('counts').alias('counts'),\n",
    "           ).alias('hist_SCINT')\n",
    "       )\n",
    "    )\n",
    "    \n",
    "    # Drift time of the events for each chamber\n",
    "    df_time = (\n",
    "    df\n",
    "    .select('ORBIT_CNT', 'chamber', 'ABSOLUTETIME')\n",
    "    .filter(col('chamber') != 4)\n",
    "    .join(df_scint, on='ORBIT_CNT', how='rightouter')\n",
    "    .filter(col('chamber').isNotNull())\n",
    "    .withColumn('DRIFTIME',\n",
    "               when(col('chamber') == 0, col('ABSOLUTETIME') - (col('t0') - time_offset_by_chamber[0]))\n",
    "              .when(col('chamber') == 1, col('ABSOLUTETIME') - (col('t0') - time_offset_by_chamber[1]))\n",
    "              .when(col('chamber') == 2, col('ABSOLUTETIME') - (col('t0') - time_offset_by_chamber[2]))\n",
    "              .when(col('chamber') == 3, col('ABSOLUTETIME') - (col('t0') - time_offset_by_chamber[3]))\n",
    "               )\n",
    "    .groupBy('chamber')\n",
    "    .agg(collect_list('DRIFTIME').alias('DRIFTIME'))\n",
    "    )\n",
    "    \n",
    "    # Total message dataframe\n",
    "    df_msg = (\n",
    "        df_counts\n",
    "        .join(df_channel, on='chamber')\n",
    "        .join(df_orbit, on='chamber')\n",
    "        .join(df_muons, on='chamber')\n",
    "        .join(df_time, on='chamber')\n",
    "        .sort('chamber')\n",
    "        .collect()\n",
    "    )\n",
    "    \n",
    "    # Paste in the dictionary message\n",
    "    tot_count = 0\n",
    "    for ch in range(0,len(df_msg)):\n",
    "        chamber = df_msg[ch]['chamber']  \n",
    "        msg[f'chamber{chamber}']['total_hits']                 = df_msg[ch]['chamber_hits']\n",
    "        msg[f'chamber{chamber}']['hist_CHANNEL']['bin_edges']  = df_msg[ch]['hist_CHANNEL']['bin_edges']\n",
    "        msg[f'chamber{chamber}']['hist_CHANNEL']['bin_counts'] = df_msg[ch]['hist_CHANNEL']['counts']\n",
    "        msg[f'chamber{chamber}']['hist_ORBIT']['bin_edges']    = df_msg[ch]['hist_ORBIT']['bin_edges']\n",
    "        msg[f'chamber{chamber}']['hist_ORBIT']['bin_counts']   = df_msg[ch]['hist_ORBIT']['counts']\n",
    "        msg[f'chamber{chamber}']['hist_SCINT']['bin_edges']    = df_msg[ch]['hist_SCINT']['bin_edges']\n",
    "        msg[f'chamber{chamber}']['hist_SCINT']['bin_counts']   = df_msg[ch]['hist_SCINT']['counts']\n",
    "        msg[f'chamber{chamber}']['DRIFTIME']                   = df_msg[ch]['DRIFTIME']\n",
    "        tot_count += df_msg[ch]['chamber_hits']\n",
    "    msg['total_hits'] = tot_count\n",
    "    \n",
    "    # Compute process time on the batch\n",
    "    end = time.perf_counter()\n",
    "    tmp = end-start\n",
    "    avg.append(tmp)\n",
    "    \n",
    "    mbmsg=sys.getsizeof(json.dumps(msg).encode('utf-8'))/(tmp*1000)\n",
    "    \n",
    "    print(f'processed batch number {batch_id} --tot_rows {tot_count} --> processed in {round(tmp, 2)} s --:{round(mbmsg, 2)} KB/s')\n",
    "     \n",
    "    producer.send(topic='topic_results', value=json.dumps(msg).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed batch number 0 --tot_rows 0 --> processed in 1.7 s --:14.17 KB/s\n",
      "processed batch number 1 --tot_rows 4641 --> processed in 2.62 s --:14.54 KB/s\n",
      "processed batch number 2 --tot_rows 9902 --> processed in 3.22 s --:23.62 KB/s\n",
      "processed batch number 3 --tot_rows 10450 --> processed in 3.64 s --:21.53 KB/s\n",
      "processed batch number 4 --tot_rows 10461 --> processed in 2.91 s --:28.45 KB/s\n",
      "processed batch number 5 --tot_rows 10625 --> processed in 3.1 s --:26.55 KB/s\n",
      "processed batch number 6 --tot_rows 9902 --> processed in 3.32 s --:22.83 KB/s\n",
      "processed batch number 7 --tot_rows 10115 --> processed in 3.12 s --:24.1 KB/s\n",
      "processed batch number 8 --tot_rows 8668 --> processed in 3.3 s --:22.63 KB/s\n",
      "processed batch number 9 --tot_rows 10259 --> processed in 3.01 s --:26.33 KB/s\n",
      "processed batch number 10 --tot_rows 10424 --> processed in 3.1 s --:25.33 KB/s\n",
      "processed batch number 11 --tot_rows 9938 --> processed in 3.02 s --:26.17 KB/s\n",
      "processed batch number 12 --tot_rows 10945 --> processed in 3.23 s --:25.82 KB/s\n",
      "processed batch number 13 --tot_rows 9863 --> processed in 3.18 s --:23.13 KB/s\n",
      "processed batch number 14 --tot_rows 9827 --> processed in 3.21 s --:24.17 KB/s\n",
      "processed batch number 15 --tot_rows 10833 --> processed in 3.0 s --:28.07 KB/s\n",
      "processed batch number 16 --tot_rows 9748 --> processed in 3.22 s --:23.33 KB/s\n",
      "processed batch number 17 --tot_rows 10381 --> processed in 3.2 s --:24.03 KB/s\n",
      "processed batch number 18 --tot_rows 10793 --> processed in 3.07 s --:28.01 KB/s\n",
      "processed batch number 19 --tot_rows 10059 --> processed in 3.03 s --:26.17 KB/s\n",
      "processed batch number 20 --tot_rows 10403 --> processed in 3.18 s --:25.72 KB/s\n",
      "processed batch number 21 --tot_rows 10597 --> processed in 2.99 s --:27.73 KB/s\n",
      "processed batch number 22 --tot_rows 10111 --> processed in 3.1 s --:25.45 KB/s\n",
      "processed batch number 23 --tot_rows 7981 --> processed in 6.22 s --:9.74 KB/s\n",
      "\n",
      "Average time taken for each batch: 3.2s\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        query = df.writeStream\\\n",
    "            .foreachBatch(process_batch)\\\n",
    "            .trigger(processingTime='4 seconds')\\\n",
    "            .option(\"checkpointLocation\", \"checkpoint\")\\\n",
    "            .start()\\\n",
    "            .awaitTermination()\n",
    "\n",
    "    except KeyboardInterrupt:   #avoid print errors\n",
    "        ##PRINT AVG PROCESS TIME\n",
    "        if len(avg) == 0:\n",
    "            break\n",
    "        tot = 0\n",
    "        for t in avg:\n",
    "            tot = tot + t\n",
    "        tot = tot/len(avg)\n",
    "        print(\"\\nAverage time taken for each batch: \" + str(round(tot, 2)) + \"s\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-master: stopping org.apache.spark.deploy.worker.Worker\n",
      "spark-slave: stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n",
      "stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.worker.Worker\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark cluster\n",
    "!$SPARK_HOME/sbin/stop-all.sh\n",
    "!ssh kafka-broker1 /usr/local/spark//sbin/stop-worker.sh\n",
    "!ssh kafka-broker2 /usr/local/spark//sbin/stop-worker.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
